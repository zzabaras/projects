{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://cdn.megabonus.com/images/shop_logo/skillbox.png\"/> \n",
    "    \n",
    "# Курс аналитик данных на Python\n",
    "## Модуль 9. Scraping/crawling данных с e-commerce сайта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас уже было небольшое введение в парсинг/scraping данных в нашем уроке по работе с библиотекой `Pandas`, когда мы делали обогащение данных для нашего отчета, но сегодня мы пойдем дальше и применим наши знания на практике, используя более продвинутую библиотку `Scrapy`. В этом уроке мы постараемся сделать нечто более сложное, а именно попробуем спарсить ассортимент и цены с сайта магазина (предположим нашего конкурента).\n",
    "\n",
    "Итак, в названии урока появились новые слова, поэтому стоит внести ясность в точки зрения терминов.<br> \n",
    "Существует 3 понятия, которые часто путают. `Parsing`, `(web)scraping` и еще добавится  `crawling`. \n",
    "\n",
    "`Parsing` - извлечение данных впринципе из любого документа (сайт/файл/логи)<br>\n",
    "`(Web)scraping` - извлечение данных с страницы сайта<br>\n",
    "`Crawling` - перемещение по сайту<br>\n",
    "Эти понятия часто смешивают вместе, тк они по-сути переплетаются между собой и следуют одно из другого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Библиотека для парсинга [`Scrapy`](https://scrapy.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Scrapy`](https://scrapy.org/) на самом деле больше чем библиотека. Это целый фреймворк, инструмент, если будет удобнее, для извлечения данных и автоматизации таких процессов.\n",
    "\n",
    "Иы же напишем скрипт, который будет решать наша задачу - мы хотим получить ассортимент по какой-либо категории с сайта и цены, а также скачаем картинки товаров (как приятный бонус). Для более детального изучения фреймфорка можно найти [здесь](https://docs.scrapy.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нужно установить библиотеку.\n",
    "\n",
    "`conda install -c conda-forge scrapy` либо `pip install scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, заходим на сайт и гуляем. Для урока я выбрал сайт [Мвидео](https://www.mvideo.ru/), а как жертва мне приглянулась категория [\"телефоны\"](https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205).<br>\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://habrastorage.org/webt/t_/s7/o4/t_s7o4wpov-_f9x5cjia12xtwh8.png\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заходим, прокручиваем первую страницу и видим, что ассортимент всех телефонов располагается на 76 страницах.<br>\n",
    "То что нужно чтобы попрактиковаться парсингу!<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://habrastorage.org/webt/xo/cc/my/xoccmyfvriikku9aw4bwc8exdxm.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Лирическое отступление или немного полезных консольных команд (bash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pwd`  ~print working directory, # печатает папку (директорию), где мы находимся в данный момент.<br>\n",
    "`ls`  ~list # показывает какие обхъекты содеражатся в папке (другие папки и файлы).<br>\n",
    "`cd имя_папки`  ~change directory # навигируем/переходим в папку, если такая имеется в директории, если нет, то выдаст ошибку.<br>\n",
    "`cd ..`  # \"шаг назад\"/ идем из текущей директории на одну выше по иерархии<br>\n",
    "`cat имя_файла` # смотрим содержимое текстового файла<br>\n",
    "\n",
    "Кстати говоря, стоит потренироваться исполнять подобные команды и в консоли. Хороший курс по этому делу можно найти [тут](https://learncodethehardway.org/unix/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем все ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_8. webscraping_ecommerce_website_with_scrapy_mvideo.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir new_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_8. webscraping_ecommerce_website_with_scrapy_mvideo.ipynb\r\n",
      "\u001b[34mnew_folder\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool/new_folder\n"
     ]
    }
   ],
   "source": [
    "cd new_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_8. webscraping_ecommerce_website_with_scrapy_mvideo.ipynb\r\n",
      "\u001b[34mnew_folder\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r new_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_8. webscraping_ecommerce_website_with_scrapy_mvideo.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Создание краулера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'mvideo', using template directory '/Users/maximkeremet/anaconda3/lib/python3.6/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool/mvideo\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd mvideo\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject mvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_8. webscraping_ecommerce_website_with_scrapy_mvideo.ipynb\r\n",
      "\u001b[34mmvideo\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заходим в папку проекта, которую мы создали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool/mvideo\n"
     ]
    }
   ],
   "source": [
    "cd mvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mmvideo\u001b[m\u001b[m/     scrapy.cfg\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно создать `spider` или краулер - набор скриптов, которые будут разворачиваться во время запуска нашего главного скрипта. Далее программа пройдет по нашей страничке и соберет все объекты, которые мы скажем ей забрать. <br>\n",
    "\n",
    "**P.S.** Называть `spider` точно так же как и проект нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'mvideo_spider' using template 'basic' in module:\r\n",
      "  mvideo.spiders.mvideo_spider\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider mvideo_spider https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видем, что наш краулер создался как модуль `mvideo.spiders.mvideo_spider`, где `mvideo` и `spiders` это папки и `mvideo_spider` это наблон нашего главного скрипта.<br>\n",
    "Зайдем в папку нашего проекта `mvideo` и посмотрим что там изменилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool/mvideo/mvideo\n"
     ]
    }
   ],
   "source": [
    "cd mvideo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py     items.py        pipelines.py    \u001b[34mspiders\u001b[m\u001b[m/\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/    middlewares.py  settings.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зайдем в папку `spiders` и увилим наш скрипт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximkeremet/courses/skillbox_analyst_course/lesson_8_webscaping_as_a_powerfiul_tool/mvideo/mvideo/spiders\n"
     ]
    }
   ],
   "source": [
    "cd spiders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py       \u001b[34m__pycache__\u001b[m\u001b[m/      mvideo_spider.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмтрим содержимое нашего скрипта, как текстового файла с помощью `cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\r\n",
      "import scrapy\r\n",
      "\r\n",
      "\r\n",
      "class MvideoSpiderSpider(scrapy.Spider):\r\n",
      "    name = 'mvideo_spider'\r\n",
      "    allowed_domains = ['https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205']\r\n",
      "    start_urls = ['http://https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205/']\r\n",
      "\r\n",
      "    def parse(self, response):\r\n",
      "        pass\r\n"
     ]
    }
   ],
   "source": [
    "cat mvideo_spider.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Разбор верстки сайта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы говорили, чтов `python` все можно рассматривать как объект. Такую аналогию можно тоже провести и с сайтом.<br>\n",
    "Интересующими объектами на нем для нас будут являться \"карточки товара\" и страницы (тоесть множества каточек товаров).<br>\n",
    "\n",
    "Вот так выглядит карточка товара:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://habrastorage.org/webt/et/qg/9y/etqg9ydvikivcy0jq5jghfc3994.png\" />\n",
    "    \n",
    "Из нее то мы как раз и будем доставать такие аттрибуты как цена, название и фотографию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наведем курсор на цену и нажмем правую кнопку мыши/область трекпада и увидим примерно такое всплывающее окно. <br>\n",
    "Нас будет интересовать пункт с промотром кода.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://habrastorage.org/webt/jl/nb/or/jlnbor-h6z60x5bpbrdiacepvz4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам откроются, так называемые инструменты разработчика.<br>\n",
    "Также в `Chrome` их можно открыть путем: `Просмотреть` -> `Разработчикам` -> `Инструменты разработчика`<br>\n",
    "Нам открылись все внутренности сайта. Тут все и разметка, и анимации `javascript` и даже трекеры, которые считают события каждого пользователя на сайте.<br>\n",
    "Заметим, что когда мы проводим курсором по строчкам кода, то каждый объект подсвечивается. Это удобно, когда мы ищем в каком объекте сайта находится наша нужная информация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ценовой блок:\n",
    "<img src=\"https://habrastorage.org/webt/9z/it/xe/9zitxeunivzgeejappffkjynl-8.png\" />\n",
    "Блок названия:\n",
    "<img src=\"https://habrastorage.org/webt/sw/vc/qf/swvcqfa2ecd9naeeyqeev5yr2fe.png\" />\n",
    "Блок с фотографией товара:\n",
    "<img src=\"https://habrastorage.org/webt/pt/2y/-8/pt2y-8k95taygnu_u1kcbbm4uu8.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном этапе нам нужно понимать, что сайт это совокупность тех или иных технологий (`html`, `css`, `js`), \n",
    "которые отвечают за то как сайт представлен для пользователя. Каждая технология отвечает за отдельную область, \n",
    "например `js` за анимирование и работой с нестатичнми структурами. <br>\n",
    "Для `webscraping`-а данных нас интересует `html` или то что написано на сайте - текстовая и числовая информация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Что такое XML и язык запросов Xpath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главное отличие **XML** от **HTML** состоит в том, что **XML** служит для хранения и передачи информации, а **HTML** служит для форматирования и отображания того же набора данных.<br>\n",
    "\n",
    "\n",
    "Что такое **XML**?\n",
    "\n",
    "* XML - аббревиатура от eXtensible Markup Language (расширяемый язык разметки).\n",
    "* XML – язык разметки, который напоминает HTML.\n",
    "* XML предназначен для передачи данных, а не для их отображения. Тоесть сам по себе он ничего не делает, а только является инструментом для транспортировки данных.\n",
    "* Теги XML не предопределены - можно определять свои собственные тэги. В HTML тэги определены и фиксированы, например: `<body>, <p>, <li>` .\n",
    "* XML описан таким образом, чтобы быть самоопределяемым.\n",
    "\n",
    "\n",
    "Так что во многих приложениях данные транспортируются с помощью **XML**, а отображаются методами **HTML**. Все вышеперечислыенные особенности **XML** делают его удобным способом взаимодействия с информацией и хорошим инструметом для наших целей.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так иерархия **XML** может быть представлена в \"древовидной\", иерархической структуре.\n",
    "<img src=\"https://habrastorage.org/webt/pp/x7/0c/ppx70czyee88a9wjyqm-3s7kw0w.gif\" width=500/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is **XPath**?<br>\n",
    "\n",
    "* XPath это специальный язык запросов, который выбирает определенные части XML документа.<br>\n",
    "\n",
    "* В языке используются указания путей до объекта, которые довольно похожи на запросы и основаны на принципе вложенности.<br>\n",
    "\n",
    "* В языке также имеются встроенные логические/числовые функции, чтобы взаимодействовать с выбираемой информацией.<br>\n",
    "\n",
    "\n",
    "Ниже приведен пример кода на **XML**, чтобы посмотреть на его структуру. С помощью **Xpath** мы можем \"забуриться\" в саму структуру и вытащить из нее интесующие нас данные, которые располагаются между тэгами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<bookstore>\n",
    "   <book category=\"COOKING\">\n",
    "      <title lang=\"en\">Everyday Italian</title>\n",
    "      <author>Giada De Laurentiis</author>\n",
    "      <year>2005</year>\n",
    "      <price>30.00</price>\n",
    "   </book>\n",
    "   <book category=\"CHILDREN\">\n",
    "      <title lang=\"en\">Harry Potter</title>\n",
    "      <author>J K. Rowling</author>\n",
    "      <year>2005</year>\n",
    "      <price>29.99</price>\n",
    "   </book>\n",
    "   <book category=\"WEB\">\n",
    "      <title lang=\"en\">XQuery Kick Start</title>\n",
    "      <author>James McGovern</author>\n",
    "      <author>Per Bothner</author>\n",
    "      <author>Kurt Cagle</author>\n",
    "      <author>James Linn</author>\n",
    "      <author>Vaidyanathan Nagarajan</author>\n",
    "      <year>2003</year>\n",
    "      <price>49.99</price>\n",
    "   </book>\n",
    "   <book category=\"WEB\">\n",
    "      <title lang=\"en\">Learning XML</title>\n",
    "      <author>Erik T. Ray</author>\n",
    "      <year>2003</year>\n",
    "      <price>39.95</price>\n",
    "   </book>\n",
    "</bookstore>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно выбирать элементы в нашем коде:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Выражение XPath | Результат   |\n",
    "|------|------|\n",
    "|   **/bookstore/book[1]**   | Выбирает первый элемент book, который является потомком элемента bookstore |\n",
    "|**/bookstore/book[last( )]**|Выбирает последний элемент book, который является потомком элемента bookstore|\n",
    "|**/bookstore/book[last( )-1]**|Выбирает предпоследний элемент book, который является потомком элемента bookstore|\n",
    "|**/bookstore/book[position( )<3]**|Выбирает первые два элемента book, которые являются потомками элемента bookstore|\n",
    "|**//title[@lang]**|Выбирает все элементы title с атрибутом lang|\n",
    "|**//title[@lang='eng']**|Выбирает все элементы title с атрибутом lang, который имеет значение 'eng'|\n",
    "|**/bookstore/book[price>35.00]**|Выбирает все элементы book, которые являются потомками элемента bookstore и которые содержать элемент price со значением больше 35.00|\n",
    "|**/bookstore/book[price>35.00]/title**|Выбирает все элементы title элементов book элементов bookstore, которые содержать элемент price со значением больше 35.00|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Catch up:** <br>\n",
    " 1) На данном этапе мы должны понимать, что текст на сайте можно получить в формате XML и к нему можно обратиться с помощью **Xpath**. \n",
    " <br>\n",
    "\n",
    " 2) **Xpath** это язык запросов, который основан на принципе вложенности тегов один в другой, образуя некую \"древовидную структуру\".Чтобы составить запрос нужно указать ту иерархию вложенности тэгов, которая нужна чтобы достать требуемые данные.<br>\n",
    "\n",
    " 3) Также, мы будем извлекать все записи по какому-либо тэгу, так что наши запросы, в основном, будут иметь вид: **//title[@lang='eng']** <br>\n",
    "<br>\n",
    " 4) Дополнительно же про XML и Xpath можно почитать [тут](https://www.w3schools.com/xml/) и посмотреть примеры запросов - [тут](https://www.tutorialspoint.com/scrapy/scrapy_extracting_items.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Разбираем сайт в консоли."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы понимаем, что и как нам надо выбирать из структуры сайта, давайте попробуем пару команд `scrapy` в консоли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(\"https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Достаем названия товаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.xpath(\"//div[@class='c-product-tile__description-wrapper']\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного дальше и вглубь по тэгам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.xpath(\"//div[@class='c-product-tile__description-wrapper']//h4/@title\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем теперь записать их в переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = response.xpath(\"//div[@class='c-product-tile__description-wrapper']//h4/@title\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно немного еще почистить и оставить только названия смартфонов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [title.split('Смартфон ')[1] for title in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Достаем картинки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши картинки лежат в том же блоке, так что можем так же записать путь в переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = response.xpath(\"//div[@class='c-product-tile-picture__link']//@data-original\").extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Достаем цены"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сам запрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разобъем на теги чтобы было визуально понятно.\n",
    "response.xpath(\"\"\"\n",
    "               //div[@class='c-product-tile__checkout-section']\n",
    "               //div[@class='c-pdp-price__current']\n",
    "               /text()\n",
    "               \"\"\").extract()\n",
    "\n",
    "# Сам запрос для scapy shell\n",
    "response.xpath(\"//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price__current']/text()\").extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем результат в переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = response.xpath(\"//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price__current']/text()\").extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока запрос очень грязный, его нужно очистить и обработать, например вот так чтобы остались только цены в числовом типе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [price.replace(\"\\n\", \"\") for price in response.xpath(\"//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price__current']/text()\").extract()]\n",
    "prices = [price.strip() for price in prices] #  удаляем лишние пробелы (удаляет и /n и /t)\n",
    "prices = [price.replace(\"¤\", \"\").replace(\"\\xa0\", \"\") for price in prices] # очищаем от остальных символов\n",
    "prices = [int(price) for price in prices if price] # преводим к int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Достаем скидки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично поступаем и со скидками, однако тут нужно применить немного хитрости.<br>\n",
    "Сам запрос выглядит примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounts = response.xpath(\"\"\".//div[@class='c-product-tile__checkout-section']\n",
    "                           //div[@class='c-pdp-price']\n",
    "                           //div[@class='c-pdp-price__sale']\n",
    "                           //span[@class='c-pdp-price__discount']\n",
    "                           /text()\"\"\").extract()\n",
    "                           \n",
    "                           \n",
    "discounts = response.xpath(\".//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price']//div[@class='c-pdp-price__sale']//span[@class='c-pdp-price__discount']/text()\").extract()                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но возникает проблема, что скидки-то не везде... В итоге мы получаем меньше записей, которые мы потом не сможет правильно сопоставить с количестом наших названий телефонов, ценами и ссылками на фотографии.\n",
    "<br>\n",
    "Придется применить хитрость. Выделим всю иерархию, которая содержит сведения о наличие скидки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounts = response.xpath(\"\"\".//div[@class='c-product-tile__checkout-section']\n",
    "                                //div[@class='c-pdp-price']\n",
    "                                //div[@class='c-pdp-price__sale']\"\"\").extract()\n",
    "\n",
    "\n",
    "# [i.xpath(\"//span[@class='c-pdp-price__discount']\").extract() if 'c-pdp-price__discount' in i else '0' for i in main_part]\n",
    "\n",
    "# full_discounts = [i.replace(\"\\n\", \"\").replace(\"\\t\", \"\") if 'c-pdp-price__discount' in i else no_discount for i in main_part.extract()]\n",
    "# action = i.replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\"\\xa0\", \"\")\n",
    "\n",
    "# [i if 'c-pdp-price__discount' in i else no_discount for i in main_part.extract()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить проблему с наличием/отсутствием скидки мы напишем функцию, которая будет брать каждую строчку (то место где у нас содержится скидка и старая цена) и очищать её от ненужных символов, а так же используем `list comprehension` чтобы задать условие, что пишем 0 для тех случаев где скидки нет.<br>\n",
    "\n",
    "Давайте посмотрим как нужно очистить строки, которые у нас получилось вытащить выше. Для этого нам понадобятся `регулярные выражения` и встроенный в `python` модуль [**`re`**](https://docs.python.org/3/library/re.html). <br>\n",
    "\n",
    "Один из самых простых подходов это пытаться найти паттерн и вычленять его. Но наш паттерн запутан и \"зашумлен\" разными артфактами, так что мы можем просто прописать правило, которое бы возвращало нам содержимое любых тэгов, обременённыв в знаки `>....<`, а далее просто очистим ее от артефактов и просто проиндексируемся по нужному нам месту в строке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "s = '<div class=\"c-pdp-price__sale\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<span class=\"u-mr-4 c-pdp-price__old\" data-sel=\"new_comparison-span-old_price-30040030-2\">\\t126\\xa0990\\n\\t</span>\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<span class=\"c-pdp-price__discount\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-9\\xa0000\\n\\t</span>\\t\\n\\t</div>'\n",
    "\n",
    "compiled = re.compile('<(.*?)>') # описываем правило что нам нужно содержимое всех тегов\n",
    "clean = re.sub(compiled, '|', s) # соединяем содержимое тэгов разделителем, для наглядности возьмем \"|\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t|\\t126\\xa0990\\n\\t|\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t|\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-9\\xa0000\\n\\t|\\t\\n\\t|'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'||126990||-9000||'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = clean.strip('').replace('\\t','').replace('\\n', '').replace('\\xa0', '') # очищаем от артефактов\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount = abs(int(clean.split('|')[4])) # индексируемся по области, где у нас находится скидка и берем положительное значение\n",
    "discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, запишем все операции в функцию чтобы было удобнее применять это к каждой строчке, которая у нас получилась в результате парсинга.<br>\n",
    "Так как в нашей строке содержатся и старая цена и скидка, то мы можем передавать дополнительно в функцию индекс соответствующего поля и тем самым написать \"универсальную\" функцию для того чтобы доставать и старую цену, и скидку (если, опять же, они существуют)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(string, field):\n",
    "    compiled = re.compile('<.*?>')\n",
    "    clean = re.sub(compiled, '|', string)\n",
    "    clean = clean.strip('').replace('\\t','').replace('\\n', '').replace('\\xa0', '')\n",
    "    amount = abs(int(clean.split('|')[field]))\n",
    "    return amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем извлекать и старую цену и скидку, например так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Старая цена: 126990 рублей.\n",
      "Скидка: 9000 рублей.\n"
     ]
    }
   ],
   "source": [
    "print('Старая цена: {} рублей.'.format(refine(s, 2)))\n",
    "print('Скидка: {} рублей.'.format(refine(s, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь применим к каждой записи, где у нас есть тэг с скидкой. Сделаем это с простого `list comprehension`, аналога цикла, в котором содержатся 2 условия и конструкция for `i in ...`, которая проверяет каждый элемент в списке на данные условия. \n",
    "<br>\n",
    "Стоит отметить, что функция возвращает нам целове число типа int, поэтому если тэга скидки нет, то будем возвращать 0.<br>\n",
    "Также, теперь мы можем вычленить как скидку, так и стару цену, в случае если таковые есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_old_prices  = [refine(discount, 2) if 'c-pdp-price__discount' in discount else np.nan for discount in discounts]\n",
    "clean_discounts  = [refine(discount, 4) if 'c-pdp-price__discount' in discount else 0 for discount in discounts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Составляем скрипт полностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Достаем сырые данные\n",
    "imgs = response.xpath(\"//div[@class='c-product-tile-picture__link']//@data-original\").extract()\n",
    "titles = response.xpath(\"//div[@class='c-product-tile__description-wrapper']//h4/@title\").extract()\n",
    "prices = response.xpath(\"//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price__current']/text()\").extract()\n",
    "discounts = response.xpath(\"//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price']//div[@class='c-pdp-price__sale']\").extract()                           \n",
    "\n",
    "# Дополняем ссылки на картинки\n",
    "imgs = ['http:'+img for img in imgs]\n",
    "\n",
    "# Обработка названий\n",
    "clean_titles = [title.split('Смартфон ')[1] for title in titles]\n",
    "\n",
    "# Обработка цен\n",
    "prices = [price.replace(\"\\n\", \"\") for price in response.xpath(\"//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price__current']/text()\").extract()]\n",
    "prices = [price.strip() for price in prices] #  удаляем лишние пробелы (удаляет и /n и /t)\n",
    "prices = [price.replace(\"¤\", \"\").replace(\"\\xa0\", \"\") for price in prices] # очищаем от остальных символов\n",
    "prices = [int(price) for price in prices if price] # преводим к int\n",
    "\n",
    "# Обработка старых цен\n",
    "clean_old_prices  = [refine(discount, 2) if 'c-pdp-price__discount' in discount else np.nan for discount in discounts]\n",
    "\n",
    "# Обработка скидок\n",
    "clean_discounts  = [refine(discount, 4) if 'c-pdp-price__discount' in discount else 0 for discount in discounts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам нужно будет записать все наши значения в словарь и потом передавать его внутренним методам `scrapy`, чтобы они записали это в формате `.csv`.<br>\n",
    "Для этого используем цикл `for` и конструкцию `zip`.<br>\n",
    "\n",
    "[Оператор `yield`](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do) это аналог `return`, только возвращает не сами значения, а генератор. Это нужно для того чтобы краулер мог сам вызывать нашу функцию, получая данные в виде словаря значений, тех полей которые мы хотим получить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in zip(clean_titles,clean_prices,old_prices,clean_discounts,imgs):\n",
    "            scraped_info = {\n",
    "                'title' : item[0],\n",
    "                'price' : item[1],\n",
    "                'old_price' : item[2],\n",
    "                'discount_offer': item[3],\n",
    "                'image_urls' : [item[4]]}\n",
    "            yield scraped_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Автоматизируем для всех страниц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно это можно сделать циклом или используя специальные структуры библиотеки, такие как `Rule` и `LinkExtractor`, <br>\n",
    "но ссылки на страницы в связном отличаются и проще оказывается можно сделать лист с страницами и краулер будет итерироваться по ним."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Составляем корректный список страниц. \n",
    "allowed_domains = ['https://www.mvideo.ru/']\n",
    "first_page = ['https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205'] \n",
    "all_others = ['https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205/f/page='+str(x) for x in range(2,77)]\n",
    "\n",
    "# Вставлем на 1 меcто (0 индекс) нашу первую страницу и все остальные далее.\n",
    "start_urls = first_page + all_others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Финишные штрихи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно в нашей папке `spiders` в нашем главном скрипте `mvideo_spider.py` следует добавить куда складывать результаты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_settings = {'FEED_URI' : 'results/mvideo.csv'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно в нашей папке проекта `mvideo` есть скрипт `settings.py`. <br>\n",
    "В нем нам нужно будет добавить несколько переменных для внутренних методов `scrapy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT_NAME = 'mvideo'\n",
    "\n",
    "SPIDER_MODULES = ['mvideo.spiders']\n",
    "NEWSPIDER_MODULE = 'mvideo.spiders'\n",
    "FEED_FORMAT = \"csv\"\n",
    "FEED_URI = \"mvideo.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А также указать что мы хотим скачивать фотографии, вот такой вот конструкцией (тоже в `settings.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "  'scrapy.pipelines.images.ImagesPipeline': 1\n",
    "}\n",
    "IMAGES_STORE = 'results/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Готово!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие скрипты нам трогать не требуется. <br>\n",
    "Теперь можно запускать скрипт из консоли, находясь в папке нашего проекта (`mvideo`) выполнив команду `scrapy crawl mvideo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Скрипт полностью "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже я собрал все части скрипта воедино. Это все можно полностью вставить в файл с названием `mvideo.py` и добавить в папку нашего краулера.<br>\n",
    "\n",
    "**_P.S._** Нужно помнить про то что отступы в скрипте должны быть однообразны (либо 4 пробела, либо tab). <br>\n",
    "Что конкретно использовать это [открытый вопрос](https://www.google.ru/search?newwindow=1&ei=W6ALXLGgJ8qcsAGLqo2gDw&q=tabs+or+4+spaces&oq=tabs+or+4+spaces&gs_l=psy-ab.3..0j0i8i30l3.5807.8172..8301...3.0..0.108.507.3j2......0....1..gws-wiz.......0i7i30j0i13j0i8i7i30j0i7i5i30j0i8i13i30.nfwBhAPIbgk) в программистком сообществе, но я предложил вам вариант с tab-ами.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scrapy\n",
    "import re\n",
    "\n",
    "\n",
    "class MvideoSpider(scrapy.Spider):\n",
    "\n",
    "\n",
    "\tcustom_settings = {'FEED_URI' : 'results/mvideo.csv'}\n",
    "\n",
    "\tname = 'mvideo'\n",
    "\n",
    "\t\"\"\" Составляем корректный список страниц. \"\"\"\n",
    "\tallowed_domains = ['https://www.mvideo.ru/']\n",
    "\tfirst_page = ['https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205'] \n",
    "\tall_others = ['https://www.mvideo.ru/smartfony-i-svyaz/smartfony-205/f/page='+str(x) for x in range(2,77)]\n",
    "\t\n",
    "\t\"\"\" Вставлем на 1 меcто (0 индекс) нашу первую страницу. \"\"\"\n",
    "\tstart_urls = first_page+all_others\n",
    "    \n",
    "\tdef refine(self, string, field):\n",
    "\t\tcompiled = re.compile('<.*?>')\n",
    "\t\tclean = re.sub(compiled, '|', string)\n",
    "\t\tclean = clean.strip('').replace('\\t','').replace('\\n', '').replace('\\xa0', '')\n",
    "\t\tamount = abs(int(clean.split('|')[field]))\n",
    "\t\treturn amount\n",
    "\n",
    "\n",
    "\tdef parse(self, response):\n",
    "\n",
    "\n",
    "\t\timgs = response.xpath(\"//div[@class='c-product-tile-picture__link']//@data-original\").extract()\n",
    "\t\ttitles = response.xpath(\"//div[@class='c-product-tile__description-wrapper']//h4/@title\").extract()\n",
    "\t\tprices = response.xpath(\"//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price__current']/text()\").extract()\n",
    "\t\tdiscounts = response.xpath(\".//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price']//div[@class='c-pdp-price__sale']\").extract()                           \n",
    "\n",
    "\t\timgs = ['https:'+img for img in imgs]\n",
    "        \n",
    "\t\tclean_titles = [title.split('Смартфон ')[1] for title in titles]\n",
    "\n",
    "\t\tprices = [price.replace(\"\\n\", \"\") for price in response.xpath(\"//div[@class='c-product-tile__checkout-section']//div[@class='c-pdp-price__current']/text()\").extract()]\n",
    "\t\tprices = [price.strip() for price in prices] #  удаляем лишние пробелы (удаляет и /n и /t)\n",
    "\t\tprices = [price.replace(\"¤\", \"\").replace(\"\\xa0\", \"\") for price in prices] # очищаем от остальных символов\n",
    "\t\tclean_prices = [int(price) for price in prices if price] # преводим к int\n",
    "\n",
    "\n",
    "\t\tclean_old_prices  = [self.refine(discount, 2) if 'c-pdp-price__discount' in discount else np.nan for discount in discounts]\n",
    "\n",
    "\t\tclean_discounts  = [self.refine(discount, 4) if 'c-pdp-price__discount' in discount else 0 for discount in discounts]\n",
    "\n",
    "\t\tfor item in zip(clean_titles, clean_prices, clean_old_prices, clean_discounts, imgs):\n",
    "\t\t\t\tscraped_info = {\n",
    "\t\t\t\t\t'title' : item[0],\n",
    "\t\t\t\t\t'price' : item[1],\n",
    "\t\t\t\t\t'old_price' : item[2],\n",
    "\t\t\t\t\t'discount_offer': item[3],\n",
    "\t\t\t\t\t'image_urls' : [item[4]]}\n",
    "\t\t\t\tyield scraped_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также у нас появилась новая непонятная штука `self` как параметр в функции `parse` и также мы ее вызываем при обработке строки.<br>\n",
    "Что это такое?<br>\n",
    "\n",
    "Мы уже знаем что всё в  `python` является объектом. Это означает, что каждый объект в `python` имеет метод и значение по той причине, что все объекты базируются на классе. Итак, класс – это проект объекта. То есть, когда мы создаем какую-либо переменную, например строку, то у нее появляется N количество методов, которые прописаны в классе `string` в `python`.\n",
    "Или например, когда мы создаем какую-либо табличку в `pandas`, то у нее есть прописанный функционал и методы, которые прописаны в классе `DataFrame` в `pandas`.<br>\n",
    "<br>\n",
    "\n",
    "Итак, чтобы взаимодействовать классам друг с другом самое первое это то, что нужен способ, что ссылаться на самих себя. Слово `self` это и есть способ описания любого объекта, буквально передающий описание себя другому классу. <br>\n",
    "Тоесть это можно прочитать следующим образом: в нашем собственном классе `MvideoSpider`, абсолютно новом обхекте есть иструкции что нужно делать - это функции `refine` или `parse`. В них мы передаем аргументы и одним из них является встроенный аттрибут `self`, который передает описание самого класса/ проекта, чтобы он мог прочитать сам себя.<br>\n",
    "\n",
    "**Catch up:** <br>\n",
    "Итак, чтобы не уходит в ООП нам нужно запомнить что в `python` как языке есть возможность реализации классов. Классы это проект, сущность языка, которая описывая абстракцию, которую может создать сам пользователь или использовать написанную/встроенную в язык. У классов есть методы и аттрибуты, как например `self`, который описывает сам класс и он нужен если мы хотим построить взаимодействие с другими структурами, например как функции. Для того чтобы вызывать функцию внутри класса нужно начать с `self.` - это вызывет функцию внутри самого класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. Ресурсы и полезные ссылки:\n",
    "\n",
    "[Документация Scrapy](https://doc.scrapy.org/en/latest/intro/overview.html)<br>\n",
    "[Базовые понятия XML и Xpath](https://www.w3schools.com/xml/xml_whatis.asp)<br>\n",
    "[Памятка по Xpath](https://blog.michaelyin.info/scrapy-tutorial-7-how-use-xpath-scrapy)<br>\n",
    "[Туториал-1](https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/)<br>\n",
    "[Туториал-2](https://blog.michaelyin.info/scrapy-tutorial-7-how-use-xpath-scrapy/)<br>\n",
    "[Как делать итерацию по страницам](http://qaru.site/questions/247879/scrapy-parsing-items-that-are-paginated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Домашнее задание:\n",
    "- Соскрейпить похожий сайт, например [Goods.ru](https://goods.ru/) или тот же [Мвидео](https://mvideo.ru) по какой-либо категории, например [велосипеды](https://goods.ru/catalog/velosipedy/) или [пылесосы](https://www.mvideo.ru/pylesosy-i-aksessuary-2428) и приложить `excel`/`csv` файл (желательно чтобы это было не меньше 1К строк) при сдаче ДЗ на платформе [Skillbox](https://skillbox.ru/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
